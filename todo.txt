### TODO

* make all tests pass
* modify wafl_llm to have a config file for the models to use/download
* add a standard variable for the dialogue history to be used in rules space
* only files declared with #using can be used in rules
* rewrite documentation
* push to main!


* retrievers for in-context learning
  /* all answerers and extractors
  /* create adversarial examples for task extractor

* make this work:
  - Do I need an umbrella?
  - Tell me if I need an umbrella

  The issue is that the relevant rule is not retrieved. The entailer gives ~0 btw the user says"do I need an umbrella" :- the user wants to know the weather
  Find a good way to entail what you need.

  - Issues were:
     - Tetriever is not very good. There are a lot other rules scooped up when asking for a task
     - Interrution rule must not be used for tasks
     - Text-generation tasks are understood as questions, therefore no text generation

* Maybe you need to add text_generation task right before searching the answer in rules

/* add feedback from frontend
* Feebdack texts should be used in-context for dialogue and task extractor
    * Use retriever to create dynamic prompts

* make the text disappar in the input after you write
* input should be frozen when the bot is thinking/speaking
* add a check for rules that are too generic.
  - For example, "the user wants to add something"
  - The rule priority should be weighed down according to
    how easy it is to trigger it from different utterances.
* better web UI
* add a way to change the rules on the fly
* create a battery of tests for questions that interrupt a question
  from the bot


/* Make tests pass
/* RETRIEVAL for prompts
/* Add RETRIEVE as a command
* add all the rules you have to the retrievable examples



* only first level for #using
* flexible config for faster answers:
   * make it so the task extractor can be skipped
   * same for task creator

* allow for facts to be checked by the llm directly
* create command line instructions "wafl list all the files"
* why does "[computer] computer what should I buy" does not trigger a rule in wafl_home?


* if utterance would trigger rule but task is unknown, then trigger the rule
* interruptions should always be called. Make it so it is impossible to forget to call them
* give the system the ability to create rules to solve tasks
* use <internal_thoughts></internal_thoughts> in prompt for dialogue

/* add interaction on lists (actions for each item in the list)
/* allow code creation from task creator
/*Does it make sense to have the task_extractor work only when the user issues a command?
/  - Use LLM only after entailment with "The user asks to do something"

/* prompt generation only if it is task
/* otherwise = should return the result of a rule that is being executed
/* implement rules in case the task has not immediate trigger

/* is the sound wave sent to wafl-llm whisper_handler correct? is it corrupted
/  - rememeber that the sound wave was corrupted the other way round0
/  - you might want to use base64 encoding for the sending part as well
/  - save sound file from whisper_handler.py and listen to it


* why is it so slow??
/- would .generate(do_sample=False) accelerate? in llm_handler.py
/  - play around with early stopping and max_length. Use eos_token_id properly

  - would shorter prompt accelerate? maybe you can retrieve the examples that are most relevant

/  - USE past_key_values as argument in generate.
/    This argument is returned when use_cache=True.
/    Save first past_key_values and then use it as argument in generate.
     -> It does not do much for a single query

/  - try to go line-by-line in the code to see where the problem is
/  - would .generate(do_sample=False) accelerate? in llm_handler.py
  - would shorter prompt accelerate? maybe you can retrieve the examples that are most relevant


/* thank you does not close the conversation because of the entailer in inference_answerer
/* refactor sentence-transformer to backend

* policy should only be about finding the correct rule to apply.
    * no y/n stuff, only choose btw candidate rules and none of the above.
    * erase y/n policy, only rule policy remains

/* refactor whisper to backend
/* refactor speaker

/* better frontend (smaller facts and choices)
/* Computer what is your name
/  -> Correct remember
/    -> answer is "I don't know". why?!
/    explanation: question -> task from question -> answer from task
/    solution: there should be a way to use the chitchat answerer to answer questions


* make it so one can change rules on the fly (reload when changed)

* add definions to arbiter (what is tea...?)

* add the ability to ask questions to activate a trigger rule.

* Make chit chat work!
  * should you take it out of arbiter? Maybe it should be the last item in ListAnswerer
    in conversational_events

* if answer is unknown in item = function() then that line should be considered False

* add main.wafl
  * it should divide by
    * the user wants to do something
    * the user asks for information
    * anything else

* add a way for the answerer to access gpt information
   * it should say "I believe ..."

* the rules should be chosen by gptj according to the prior conversation
  (possibly from bw_inference _look_for_answer_in_rules)

* add docker compose for running server + interface
  * add a flask interface for api

* create errors within parser
   * dependency does not exist
   * . is allowes in dependency, / is not

* Clean conversation summary
  * [computer] is in the summary
  * Sorry? is in the summary
  * repetitions are in the summary
  * sentences that are said by the bot are treated the same as they were said by the user
  * maybe questions can be asked at a summary level? Not just sentence by sentence.

* rules should be added and REMOVED!

/* all sentences that are said when the interface is deactivated should not be appended to the conversation

/* Write choices, tasks and remember in web interface
/* erase conversation after deactivation in voice interface
/* refactor everyting to have a chat log with all the choices, retrieved facts
  in the same prompt for chitchat
  /* All answerers should have access to the same list of utterances, choices, and facts.
    The only difference should be the final line of prompt.
    /* MAKE TEST PASS
    /* ADD CACHE TO ALL CONNECTORS (it cannot be done easily for async functions)

/* 1024 is limit in wafl_llm (deepspeed) change this to 2048
/* make it sure 2048 is limit when using gptj
/* only last three conversation items in task extractor

/* Complete the task extractor + policy guidance on the whole of the answerer/inference tree
/  * Create test for different policies
/     * create test for "I didn't mean that"
/        * You need to log the rules that have been chosen.
           This needs to part of the policy decision.
           Dialogue alone is not enough.

/     * create test for "do it again"
       * FIND WAY TO EXTRACT TASK FROM DISCOURSE in arbiter_answerer
       /* Selected_answer() returns unknown if all answers are None
       /* add task recognition to choices and make tests pass

/* make alarm work on wafl_home.
/  * Make async work on wafl
/  * Do not forget to reinstate "what time is it" rule

/* Try to use only GPTJ. Answer questions with dialogue and story.
  Give few examples where the answer is unknown.

/* volume threshold should be sampled continously

/* change {"%%"} into something more typeable (possibly "% %")
/* add torchserve handler as a wafl init

/* Add GENERATE
/    * Use it to get "1" from "one minute"

/* time needs to be pre-processed to trigger rules (5 past seven -> 7,05)
/* add event loop here to test_scheduler
/* create way to add rules


This and next week:
 /* rules in folder
 /* distill gpt2 from gpt-JT onto the CoQA dataset
    -> not done, using gpt-jt directly

First week of the year:
 /* Make test conversations work
 * Debugging with picture as output
   -> not doing it now

Second week of the year:
 * User-defined events
 /* Scheduler

Third week of the year:
 * Refactoring
 * Write up docs

Fourth week:
 * Write demo paper
   * Demo paper should include website with code editor (and connection to GPU)


/* rules and functions should be in a folder.
/  * Think about ability to install

/* add ability to create rules through text "->"
* main conversation loop should be scheduler
  * Add functions that can trigger rules

* InferenceAnswerer can be broken down into simpler answerer
  * within backward inference there is the need for an answerer (when tasks are interrupted)

* Do lists as hard-coded
* y/n questions are never searched in working memory. Is this the right behavior?
/* test_testcases blocks the tests. RESOLVE THIS!!
* fine tune entailer to the tasks in this systems (the bot says: "", the user asks ""...)
* functions use inference from depth_level = 1. This can induce infinite recursion

* if query is not question and answer is False then the system should say "I cannot do it because"
   * After because there should be the answer to the bot asking itself why
   * The way to do it is to have a narrator connected to the logs
   * you need better readable logs
   * you need a way to translate the logs into a coherent text
   * THIS WILL ADD INTROSPECTION!!

* move all thresholds to variables.py
* rules are sorted by retriever but not by entailer!! Do that

* add error detection in parser.
  * for example ( without a closing ). same for {


/* find way to connect to local ngrok from github
/* entailment should be :- instead of <-
/* take entailer and qa out of the __init__ in entailer.py and qa.py
/* implement functions within each dependency
/* remove Batches output in prediction
/* fact retrieval should work in python space as well
/* y/n questions should only accept yes or no (and loop if there is something else)
/* Why does remember not work??!!!
/* separate items added to list with "and"
/* The same answer cannot belong to more than one question in the same task! (it's an approx but needed)
/* Create answering class on top of the conversation
/  * create arbiter class
/    * This should solve the test_executables failing tests
/  * is the dialogue part really needed? if not, you can use a simple qa system

/* Use entailer for common sense? Creak sense does not work very well
/* Make infinite recursion impossible (set max limit or check for repetitions)
/* Do the conversational memory (start with test_working_memory.py)
/ * done but you need to refine the interaction with the narrator class (events are splitted manually in qa.py)
/    * RUN TESTS!!!
/    * START WITH test_conversation (many tests are failing)
/        * The issue is in "the user says" (line 85 in qa.py)
/            - The system should be able to understand if the the user is speaking or the user
/            - possibly if the question is from the user (like in working memory) add "user says"
/            - what would you add when the fact comes from the knowledge base?
/                - should you change the hypothesys "when -> says?" (line 91 and 101)
/
/    * USE LOGGER IN CONVERSATION() TO SPEED UP DEBUGGING!
/* numbers in speaker should be translated to English
/* remove computer as first word
/* Confidence in listener results
/* lists should filter the items
/* a list cannot contain itself, you should check the name
/* Yes/No questions should be more flexible
/* voice thresholds should be in config (write test about them)
/* ADD VERSION NUMBER WHEN STARTING UP
/* check tests.test_working_memory.TestWorkingMemory.test_working_memory_works_for_yes_questions
/* computer name triggers a "faulty" sound

/* functions.py need a more clever way of handling hidden arguments
  - should all functions have a hidden arg?
  - make it possible to have more files

* use different voice model (this one from hf?  )

* speech to text
    - use your own beam decoder + n-gram to improve quality
    - use newer model?
    - filter out filler sounds

* Unify conversation/utils.py "the user says/asks" and the presupposition replace "to the bot" in entailer.py

* Detect who is talking to whom. Some rules can only be activated by the user speaking to the bot
    - Use get_sequence_probability_given_prompt()

* add in knowledge facts about "the user". You don't need to remember everything that is said

* Is there a need for a "Main Task" ? One that oversees everything?

* Python hooks need to be in a class, like with Tests

* rms threshold should be average of background noise
* add delete last item


* Change the speaker voice

* Create unit tests for conversation activation/deactivation

* The answer to the question can be found in the conversation from the bot.
  The bot can speak to itself and then answer the user

* train your own retriever using the conversations in dailydialogue? MAYBE NOT
* If the query is a question YOU NEED A QA RETRIEVER. take MULTI_QA instead of MSMARCO
  - also if the rule.effect is a question


* If a function calls another one within functions.py then there is an argument missing! inference is not there in the code

*yes/no questions should *never* trigger an interruption.
 It's not just yes/no answers, the deal is with the question!



* Select by levenstein distance before text_retriever (lev_retriever?)
* train bart for qa/facts
* train your own retriever

* Why is everything interpreted as "hello" or "hi"? You need a better retriever
  - Change retriever with the one you liked


* allow {variable} to be interpreted as code/call for another task (at least add tests)
* allow some type of introspection

* finish config
 - add hotwords to config.json
* upload to github, with tests and logs
* validate the user code (is REMEMBR spelled correctly?)

* should yes/no filter be in retriever instead of knowledge?

* Create interfaces for google voice, other apis
* A goal oriented bot would scan all the rules to find how to obtain the goal.
  The user can be simulated using a generative model for dialogue.

* for yes/no or limited choice questions there should not be ambiguity. The machine should match the closest item and
  if there is no item close enough ask again.


* New voice! It's ridiculous to have to have a memory leak from picotts
    - Use fairseq voices

* why is Alberto not recognized as a name by the retriever? need a better retriever than MSMARCO distill
* Working memory should really be working knowledge
* refactor BackwardInference

!* make it so if the user does not know the answer, one can continue inference?
   - Or should you try to do the inference first??
!* Implement a standard sign for code. Should it be '''> ?
!* Implement FORGET (the whole Fact should disappear from Knowledge)


* Do not allow arbitrary input (at least for voice)
* Working memory is unnecessarily complicated. It can just contain the story and some method to automatically fill it.
!* Why did it say "no" on "can you please add bananas to the shopping list?"
* Better QA for yes/no question (maybe add SNLI to qa?)
* Add math expressions to fact-checker (some, any, every)
* USER REQUEST: Multiple items to the shopping list in one go: apples AND bananas AND vegetables
* create tests for voice
* Parser should allow for empty lines within rules
* Implement Server with HTML page (docker-compose up)
* Refactor code and clean up

* Investigate interplay btw substitutions and already_matched
  - Maybe one can avoid having same answer for btw same question and same depth (and same rule)
* Say "This is true" or "this is false" if a statement matches (My name is Alberto -> True)
* working memory only within the same level of rules with same activation
  GET WORKING MEMORY FOR FACTS WORKING WITH SHOPPING LIST!!

/* Add working memory for python-space
    - Maybe exclude WM answer if it is the same as the prior answer
/* Use entailment to make generated qa more accurate
    /- Upload qa system to huggingface and pip
    /-**** Use conversation_qa -> refactor qa.py

